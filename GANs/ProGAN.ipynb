{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 33, 16, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MiniBatchStd(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        avg_std = torch.std(x, dim=0).mean().item()\n",
    "        feature_map = torch.full((batch_size, 1, height, width), avg_std, device=x.device)\n",
    "        out = torch.cat([x, feature_map], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = MiniBatchStd().to(device)\n",
    "x = torch.randn(8, 32, 16, 16).to(device)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvEqualizedLR(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, is_transpose=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding) if not is_transpose else nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        fan_in_factor = np.sqrt(in_channels * kernel_size * kernel_size)\n",
    "        gain_factor = np.sqrt(2)\n",
    "        self.norm_constant = gain_factor / fan_in_factor\n",
    "\n",
    "        nn.init.normal_(self.conv.weight.data, 0, 1)\n",
    "        nn.init.constant_(self.conv.bias.data, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x * self.norm_constant)        \n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.randn(8, 3, 16, 16).to(device)\n",
    "model = ConvEqualizedLR(3, 32, 3, 1, 1).to(device)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PixelwiseNorm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_of_squares = torch.mean(x**2, dim=1, keepdim=True)\n",
    "        factor = torch.sqrt(mean_of_squares + self.epsilon)\n",
    "        out = x / factor\n",
    "\n",
    "        return out\n",
    "    \n",
    "model = PixelwiseNorm().to(device)\n",
    "x = torch.randn(8, 3, 16, 16).to(device)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 8, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv3x3Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        pn = PixelwiseNorm()\n",
    "        act = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        self.conv_eq1 = nn.Sequential(\n",
    "            ConvEqualizedLR(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            pn,\n",
    "            act\n",
    "        )\n",
    "        self.conv_eq2 = nn.Sequential(\n",
    "            ConvEqualizedLR(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            pn,\n",
    "            act\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_eq1(x)\n",
    "        out = self.conv_eq2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "x = torch.randn(8, 512, 8, 8).to(device)\n",
    "model = Conv3x3Block(512, 512).to(device)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 8, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ToRGBLayer(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.rgb_layer = ConvEqualizedLR(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rgb_layer(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.randn(8, 512, 8, 8).to(device)\n",
    "model = ToRGBLayer(512).to(device)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 16, 16])\n",
      "torch.Size([8, 3, 16, 16])\n",
      "torch.Size([8, 3, 16, 16])\n",
      "torch.Size([8, 3, 16, 16])\n",
      "torch.Size([8, 3, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmoothFadeIn(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, alpha, upsampled_rgb, learnt_upsampled_rgb):\n",
    "        interpolated_img = (1-alpha)*upsampled_rgb + alpha*learnt_upsampled_rgb\n",
    "        out = self.act(interpolated_img)\n",
    "\n",
    "        return out\n",
    "\n",
    "upsampled_rgb = torch.rand(8, 3, 16, 16).to(device)\n",
    "learnt_upsampled_rgb = torch.rand(8, 3, 16, 16).to(device)\n",
    "fadein_layer = SmoothFadeIn().to(device)\n",
    "for alpha in torch.linspace(0, 1, 5):\n",
    "    out = fadein_layer(alpha, upsampled_rgb, learnt_upsampled_rgb)\n",
    "    print(out.shape)\n",
    "(out == nn.Tanh()(learnt_upsampled_rgb)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 4, 4])\n",
      "torch.Size([8, 3, 8, 8])\n",
      "torch.Size([8, 3, 16, 16])\n",
      "torch.Size([8, 3, 32, 32])\n",
      "torch.Size([8, 3, 64, 64])\n",
      "torch.Size([8, 3, 128, 128])\n",
      "torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class ProGenerator(nn.Module):\n",
    "    def __init__(self, channel_factors, latent_dim=512, starting_channels=512, img_channels=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.smooth_fadein = SmoothFadeIn()\n",
    "\n",
    "        self.first_block = nn.Sequential(\n",
    "            ConvEqualizedLR(latent_dim, starting_channels, 4, 1, 0, is_transpose=True),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            ConvEqualizedLR(starting_channels, starting_channels, 3, 1, 1),\n",
    "            PixelwiseNorm(),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "        )\n",
    "        self.first_torgb = ToRGBLayer(starting_channels, img_channels)\n",
    "        self.block_modules, self.rgb_modules = nn.ModuleList([self.first_block]), nn.ModuleList([self.first_torgb])\n",
    "\n",
    "        for idx in range(len(channel_factors) - 1):\n",
    "            in_channels, out_channels = int(starting_channels * channel_factors[idx]), int(starting_channels * channel_factors[idx+1])\n",
    "            self.block_modules.append(Conv3x3Block(in_channels, out_channels))\n",
    "            self.rgb_modules.append(ToRGBLayer(out_channels, img_channels))\n",
    "\n",
    "\n",
    "    def forward(self, x, resolution_idx, alpha):\n",
    "        learnt_upsampled = self.first_block(x)\n",
    "        \n",
    "        if resolution_idx == 0:\n",
    "            learnt_upsampled_rgb = self.first_torgb(learnt_upsampled)\n",
    "            return learnt_upsampled_rgb\n",
    "        \n",
    "        elif resolution_idx > 0:\n",
    "             \n",
    "            for idx in range(resolution_idx):\n",
    "                upsampled = self.upsample(learnt_upsampled)\n",
    "                learnt_upsampled = self.block_modules[idx+1](upsampled)\n",
    "                        \n",
    "            upsampled_rgb = self.rgb_modules[resolution_idx - 1](upsampled)\n",
    "            learnt_upsampled_rgb = self.rgb_modules[resolution_idx](learnt_upsampled)\n",
    "            \n",
    "            learnt_upsampled_rgb = self.smooth_fadein(alpha, upsampled_rgb, learnt_upsampled_rgb)\n",
    "            \n",
    "            return learnt_upsampled_rgb\n",
    "\n",
    "x = torch.rand(8, 512, 1, 1).to(device)\n",
    "channel_factors = [1, 1, 1, 1, 1/2, 1/4, 1/8]\n",
    "model = ProGenerator(channel_factors).to(device)\n",
    "img_res = [4, 8, 16, 32, 64, 128, 256]\n",
    "resolution_indices = [int(np.log2(res/4)) for res in img_res]\n",
    "for res_idx in resolution_indices:\n",
    "    output = model(x, res_idx, 0.5)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FromRGBLayer(nn.Module):\n",
    "    def __init__(self, out_channels, img_channels=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.rgb_layer = ConvEqualizedLR(img_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rgb_layer(x)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.randn(8, 3, 8, 8).to(device)\n",
    "model = FromRGBLayer(512).to(device)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProDiscriminator(nn.Module):\n",
    "    def __init__(self, channel_factors, ending_channels=512, img_channels=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.downsample = nn.AvgPool2d(2)\n",
    "        self.smooth_fadein = SmoothFadeIn()\n",
    "\n",
    "        self.block_modules, self.rgb_modules = nn.ModuleList([]), nn.ModuleList([])\n",
    "\n",
    "        for idx in range(len(channel_factors) - 1, 0, -1):\n",
    "            in_channels, out_channels = int(ending_channels * channel_factors[idx]), int(ending_channels * channel_factors[idx-1])\n",
    "            self.rgb_modules.append(FromRGBLayer(in_channels, img_channels)) \n",
    "            self.block_modules.append(Conv3x3Block(in_channels, out_channels))\n",
    "\n",
    "        self.last_fromrgb = FromRGBLayer(ending_channels, img_channels)\n",
    "        self.minibatch_std = MiniBatchStd()\n",
    "        self.last_block = nn.Sequential(\n",
    "            ConvEqualizedLR(ending_channels+1, ending_channels, kernel_size=3, stride=1, padding=1),\n",
    "            PixelwiseNorm(),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            ConvEqualizedLR(ending_channels, ending_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "        )\n",
    "        self.block_modules.append(self.last_block), self.rgb_modules.append(self.last_fromrgb)\n",
    "        self.fc = nn.Linear(ending_channels*1*1, 1)\n",
    "\n",
    "    def forward(self, x, resolution_idx, alpha):\n",
    "        module_index = - resolution_idx - 1\n",
    "        upsampled = self.rgb_modules[module_index](x)\n",
    "        \n",
    "        if resolution_idx > 0:\n",
    "            learnt_upsampled = self.block_modules[module_index](upsampled)\n",
    "            learnt_downsampled = self.downsample(learnt_upsampled)\n",
    "            \n",
    "            downsampled_rgb = self.downsample(x)\n",
    "            downsampled = self.rgb_modules[module_index + 1](downsampled_rgb)\n",
    "            \n",
    "            learnt_downsampled = self.smooth_fadein(alpha, downsampled, learnt_downsampled)\n",
    "            \n",
    "            for idx in range(module_index+1, -1):\n",
    "                learnt_downsampled = self.block_modules[idx](learnt_downsampled)\n",
    "                learnt_downsampled = self.downsample(learnt_downsampled)\n",
    "\n",
    "            added_featuremap = self.minibatch_std(learnt_downsampled)\n",
    "            learnt_downsampled = self.last_block(added_featuremap)\n",
    "\n",
    "            flattened = learnt_downsampled.view(x.shape[0], -1)\n",
    "            out = self.fc(flattened)\n",
    "\n",
    "            return out\n",
    "        \n",
    "        elif resolution_idx == 0:\n",
    "            added_featuremap = self.minibatch_std(upsampled)\n",
    "            learnt_downsampled = self.last_block(added_featuremap)\n",
    "\n",
    "            flattened = learnt_downsampled.view(x.shape[0], -1)\n",
    "            out = self.fc(flattened)\n",
    "            return out\n",
    "        \n",
    "        \n",
    "x = torch.randn(8, 3, 16, 16).to(device)\n",
    "channel_factors = [1, 1, 1, 1, 1/2, 1/4, 1/8]\n",
    "model = ProDiscriminator(channel_factors).to(device)\n",
    "output = model(x, 2, 0.5)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(8, 512, 1, 1).to(device)\n",
    "# channel_factors = [1, 1, 1, 1, 1/2, 1/4, 1/8]\n",
    "# model_gen = ProGenerator(channel_factors).to(device)\n",
    "# model_disc = ProDiscriminator(channel_factors).to(device)\n",
    "# img_res = [4, 8, 16, 32, 64, 128, 256]\n",
    "# resolution_indices = [int(np.log2(res/4)) for res in img_res]\n",
    "# for res_idx in resolution_indices:\n",
    "#     output = model_gen(x, res_idx, 0.5)\n",
    "#     score = model_disc(output, res_idx, 0.5)\n",
    "#     print(output.shape, score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = '/mnt/c/Users/121js/OneDrive/Desktop/TorchImages/dogs/'\n",
    "num_channels = 3\n",
    "\n",
    "class DogsDataset(Dataset):\n",
    "    def __init__(self, data_path, image_resolution, num_channels=num_channels):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.list_filenames = os.listdir(data_path)\n",
    "\n",
    "        self.resolution_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_resolution, image_resolution)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean = [0.5 for _ in range(num_channels)],\n",
    "                    std = [0.5 for _ in range(num_channels)]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_filename = self.list_filenames[index]\n",
    "        image_path = os.path.join(self.data_path, image_filename)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        return self.resolution_transform(image)\n",
    "    \n",
    "# train_dataset = DogsDataset(root_path+'mix', 6)\n",
    "# val_dataset = DogsDataset(root_path+'beagle', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer_d,\n",
    "            optimizer_g,\n",
    "            scaler_d,\n",
    "            scaler_g,\n",
    "            model_d,\n",
    "            model_g,\n",
    "            penalty_coeff,\n",
    "            epsilon_drift,\n",
    "            device = device\n",
    "    ):\n",
    "        self.optimizer_d = optimizer_d\n",
    "        self.optimizer_g = optimizer_g\n",
    "        self.scaler_d = scaler_d\n",
    "        self.scaler_g = scaler_g\n",
    "        self.model_d = model_d\n",
    "        self.model_g = model_g\n",
    "        self.penalty_coeff = penalty_coeff\n",
    "        self.epsilon_drift = epsilon_drift\n",
    "        self.device = device\n",
    "\n",
    "    def calc_grad_penalty(self, resolution_index, alpha, real, fake):\n",
    "        batch_size, num_channels, height, width = real.shape\n",
    "        epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, num_channels, height, width).to(self.device)\n",
    "\n",
    "        joint_distribution = epsilon*real + (1-epsilon)*fake\n",
    "        critic_term = self.model_d(joint_distribution, resolution_index, alpha)\n",
    "\n",
    "        gradient = torch.autograd.grad(\n",
    "            outputs = critic_term,\n",
    "            inputs = joint_distribution,\n",
    "            grad_outputs = torch.ones_like(critic_term),\n",
    "            retain_graph = True,\n",
    "            create_graph = True,\n",
    "        )[0].view(batch_size, -1)\n",
    "\n",
    "        l2_norm = torch.norm(gradient, p=2, dim=1)\n",
    "        grad_penalty = torch.mean((l2_norm - 1)**2)        \n",
    "\n",
    "        return grad_penalty\n",
    "    \n",
    "    def calc_disc_loss(self, resolution_index, alpha, real, fake, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            real_arg = self.model_d(real, resolution_index, alpha)\n",
    "            fake_arg = self.model_d(fake, resolution_index, alpha)\n",
    "\n",
    "            grad_penalty = self.calc_grad_penalty(resolution_index, alpha, real, fake)\n",
    "\n",
    "            wgan_gp_loss = (torch.mean(fake_arg) - torch.mean(real_arg)) + self.penalty_coeff*grad_penalty\n",
    "\n",
    "            drift_term = self.epsilon_drift * torch.mean(real_arg**2)\n",
    "\n",
    "            loss_d = wgan_gp_loss + drift_term\n",
    "\n",
    "        return loss_d\n",
    "    \n",
    "    def calc_gen_loss(self, resolution_index, alpha, fake, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            fake_arg = self.model_d(fake, resolution_index, alpha)\n",
    "            loss_g = - torch.mean(fake_arg)\n",
    "\n",
    "        return loss_g\n",
    "    \n",
    "    def calc_metrics(self, resolution_index, alpha, metrics_dict, fake, train_loader, val_loader):\n",
    "        self.model_d.eval(), self.model_g.eval()\n",
    "\n",
    "        final_str = ''\n",
    "        loaders_list = [('Train', train_loader), ('Val', val_loader)]\n",
    "            \n",
    "        if metrics_dict == None:\n",
    "            metrics_dict = {'Train': {'DiscLoss': [], 'GenLoss': []}, 'Val': {'DiscLoss': [], 'GenLoss': []}}\n",
    "\n",
    "        for name, loader in loaders_list:\n",
    "            len_data = 0\n",
    "            total_loss_d, total_loss_g = 0, 0\n",
    "\n",
    "            for real in loader:\n",
    "                real = real.to(device)\n",
    "\n",
    "                batch_size = real.shape[0]\n",
    "                len_data += batch_size\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_d = self.calc_disc_loss(resolution_index, alpha, real, fake, is_train=False)\n",
    "                total_loss_d += loss_d\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_g = self.calc_gen_loss(resolution_index, alpha, fake, is_train=False)\n",
    "                total_loss_g += loss_g\n",
    "                \n",
    "            disc_loss = total_loss_d/len_data\n",
    "            gen_loss = total_loss_g/len_data\n",
    "\n",
    "            final_str += ' -- {} Disc Loss: {:.5f} -- {} Gen Loss: {:.5f}'.format(name, disc_loss, name, gen_loss)\n",
    "                \n",
    "            metrics_dict[name]['DiscLoss'].append(disc_loss.item())\n",
    "            metrics_dict[name]['GenLoss'].append(gen_loss.item()) \n",
    "\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        return final_str, metrics_dict\n",
    "    \n",
    "    def visualize_tensorboard(self, real, fake, loss_g, loss_d, writer_progan, tensorboard_steps):\n",
    "        self.model_g.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            combined_grid = torch.cat([\n",
    "                fake[:4]*0.5 + 0.5,\n",
    "                real[:4]*0.5 + 0.5,\n",
    "            ], dim=0\n",
    "            )\n",
    "            image_grid = make_grid(combined_grid, nrow=4, normalize=False)\n",
    "            writer_progan.add_image('Generated', image_grid, global_step=tensorboard_steps)\n",
    "\n",
    "            writer_progan.add_scalar('Generator Loss', loss_g.item(), global_step=tensorboard_steps)\n",
    "            writer_progan.add_scalar('Discriminator Loss', loss_d.item(), global_step=tensorboard_steps)\n",
    "\n",
    "        self.model_g.train()\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def fit_at_1_res(self, resolution_index, alpha_arr, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps):\n",
    "        metrics_dict = None        \n",
    "        epoch_loop = tqdm(range(1, n_epochs+1), total=n_epochs, leave=True)\n",
    "        for epoch in epoch_loop:\n",
    "            # batch_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "            batch_loop = enumerate(train_loader)\n",
    "            for batch_idx, real in batch_loop:\n",
    "                real = real.to(device)\n",
    "                \n",
    "                batch_size = real.shape[0]\n",
    "                noise_dim = 512\n",
    "                noise = torch.randn(batch_size, noise_dim, 1, 1).to(self.device)\n",
    "\n",
    "                alpha = alpha_arr[epoch-1, batch_idx]\n",
    "                fake = self.model_g(noise, resolution_index, alpha)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_d = self.calc_disc_loss(resolution_index, alpha, real, fake, is_train=True)\n",
    "                    \n",
    "                self.optimizer_d.zero_grad()\n",
    "                self.scaler_d.scale(loss_d).backward(retain_graph=True)\n",
    "                self.scaler_d.step(self.optimizer_d)\n",
    "                self.scaler_d.update()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_g = self.calc_gen_loss(resolution_index, alpha, fake, is_train=True)\n",
    "\n",
    "                self.optimizer_g.zero_grad()\n",
    "                self.scaler_g.scale(loss_g).backward(retain_graph=True)\n",
    "                self.scaler_g.step(self.optimizer_g)\n",
    "                self.scaler_g.update()\n",
    "\n",
    "                epoch_loop.set_description(epoch_desc)\n",
    "                epoch_loop.set_postfix(batch = f'{batch_idx+1}/{len(train_loader)}', train_loss_discriminator = f'{loss_d.item():.4f}', train_loss_generator = f'{loss_g.item():.4f}')\n",
    "\n",
    "                if batch_idx % 5 == 0:\n",
    "                    # print(f'Epoch: {epoch:2d}/{n_epochs} -- Batch: {batch_idx+1:3d}/{len(train_loader)}' + f' -- Train Disc Loss: {loss_d:.4f} -- Train Gen Loss: {loss_g:.4f}')\n",
    "                    self.visualize_tensorboard(real, fake, loss_g, loss_d, writer_progan, tensorboard_steps)\n",
    "                    tensorboard_steps += 1\n",
    "            \n",
    "            # final_str, metrics_dict = self.calc_metrics(resolution_index, alpha, metrics_dict, fake, train_loader, val_loader)\n",
    "            # print('Epoch: {:2d}'.format(epoch) + final_str)\n",
    "            \n",
    "        return metrics_dict, tensorboard_steps\n",
    "    \n",
    "    def fadein_fit(self, resolution_index, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps):\n",
    "        alpha_arr = np.linspace(0, 1, n_epochs*len(train_loader)).reshape(n_epochs, len(train_loader))\n",
    "        metrics_dict, tensorboard_steps = self.fit_at_1_res(resolution_index, alpha_arr, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\n",
    "\n",
    "        return metrics_dict, tensorboard_steps\n",
    "    \n",
    "    def stable_fit(self, resolution_index, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps):\n",
    "        alpha_arr = np.ones((n_epochs, len(train_loader)))\n",
    "        metrics_dict, tensorboard_steps = self.fit_at_1_res(resolution_index, alpha_arr, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\n",
    "\n",
    "        return metrics_dict, tensorboard_steps\n",
    "    \n",
    "    def get_data_loader(self, resolution, path, batch_size, shuffle):\n",
    "        num_channels = 3\n",
    "        data = DogsDataset(path, resolution, num_channels)\n",
    "        dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        return dataloader\n",
    "        \n",
    "    def fit(self, img_res, epochs_per_res, batches_per_res, train_path, val_path, writer_progan):\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        metrics_per_res = dict(zip(img_res, [None]*len(img_res)))\n",
    "\n",
    "        tensorboard_steps = 1\n",
    "\n",
    "        for res, n_epochs, batch_size in zip(img_res, epochs_per_res, batches_per_res):\n",
    "            \n",
    "            train_loader = self.get_data_loader(res, train_path, batch_size, shuffle=True)\n",
    "            val_loader = self.get_data_loader(res, val_path, batch_size, shuffle=False)\n",
    "\n",
    "            metrics_dict = metrics_per_res[res]\n",
    "\n",
    "            res_idx = int(np.log2(res/4))\n",
    "            if res == 4:\n",
    "                epoch_desc = f'Initial training at Image Resolution: {res}x{res}'\n",
    "                metrics_dict, tensorboard_steps = self.fadein_fit(res_idx, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\n",
    "            elif res > 4:\n",
    "                epoch_desc = f'Fade-in training at Image Resolution: {res}x{res}'\n",
    "                metrics_dict, tensorboard_steps = self.fadein_fit(res_idx, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\n",
    "                epoch_desc = f'Stable training at Image Resolution: {res}x{res}'\n",
    "                metrics_dict, tensorboard_steps = self.stable_fit(res_idx, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\n",
    "                \n",
    "            metrics_per_res[res] = metrics_dict\n",
    "\n",
    "            \n",
    "\n",
    "        self.metrics_per_res = metrics_per_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_factors = [1, 1, 1, 1, 1/2, 1/4, 1/8]\n",
    "model_d = ProDiscriminator(channel_factors, ending_channels=512).to(device)\n",
    "model_g = ProGenerator(channel_factors, latent_dim=512, starting_channels=512).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "beta1 = 0.0\n",
    "beta2 = 0.999\n",
    "optimizer_d = optim.Adam(model_d.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_g = optim.Adam(model_g.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "scaler_d = torch.cuda.amp.GradScaler()\n",
    "scaler_g = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_coeff = 10\n",
    "epsilon_drift = 1e-3\n",
    "trainer = Trainer(optimizer_d, optimizer_g, scaler_d, scaler_g, model_d, model_g, penalty_coeff, epsilon_drift, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b17f18b99a41e788f019c84444c4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training at Image Resolution: 4x4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dff06c6b2954e1b9f209f1139307e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c1aa053805460684d8d3bc5c516b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training at Image Resolution: 8x8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d5c5f6f0c34d0cb6bca18d53aaf9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eaa92c62ad4ddda6a80368a7951251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training at Image Resolution: 16x16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa74b56772e448ea1cc47ffe7c675bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m train_path, val_path \u001b[38;5;241m=\u001b[39m root_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmix\u001b[39m\u001b[38;5;124m'\u001b[39m, root_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeagle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m writer_progan \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/progan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_per_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches_per_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_progan\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 208\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, img_res, epochs_per_res, batches_per_res, train_path, val_path, writer_progan)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    207\u001b[0m     epoch_desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFade-in training at Image Resolution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 208\u001b[0m     metrics_dict, tensorboard_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfadein_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_progan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     epoch_desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStable training at Image Resolution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    210\u001b[0m     metrics_dict, tensorboard_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstable_fit(res_idx, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\n",
      "Cell \u001b[0;32mIn[14], line 171\u001b[0m, in \u001b[0;36mTrainer.fadein_fit\u001b[0;34m(self, resolution_index, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfadein_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, resolution_index, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps):\n\u001b[1;32m    170\u001b[0m     alpha_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\u001b[38;5;241m.\u001b[39mreshape(n_epochs, \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m--> 171\u001b[0m     metrics_dict, tensorboard_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_at_1_res\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolution_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_progan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics_dict, tensorboard_steps\n",
      "Cell \u001b[0;32mIn[14], line 145\u001b[0m, in \u001b[0;36mTrainer.fit_at_1_res\u001b[0;34m(self, resolution_index, alpha_arr, n_epochs, epoch_desc, train_loader, val_loader, writer_progan, tensorboard_steps)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_d\u001b[38;5;241m.\u001b[39mscale(loss_d)\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler_d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_d\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n",
      "File \u001b[0;32m~/miniconda3/envs/PyTorchGPU/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/PyTorchGPU/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/PyTorchGPU/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_res = [4, 8, 16, 32, 64, 128, 256]\n",
    "epochs_per_res = [80, 90, 70, 50, 50, 50, 50]\n",
    "batches_per_res = [64, 64, 32, 32, 16, 16, 8]\n",
    "root_path = '/mnt/c/Users/121js/OneDrive/Desktop/TorchImages/dogs/'\n",
    "train_path, val_path = root_path+'mix', root_path+'beagle'\n",
    "writer_progan = SummaryWriter('logs/progan')\n",
    "\n",
    "trainer.fit(img_res, epochs_per_res, batches_per_res, train_path, val_path, writer_progan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
