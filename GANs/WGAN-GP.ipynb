{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, is_last=False, use_bn=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity() \n",
    "        self.act = nn.Tanh() if is_last else nn.ReLU()       \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed = nn.Embedding(10, 100)\n",
    "        # self.fc = nn.Linear(100*2, 1024*4*4)\n",
    "        self.first_conv = UpConvBlock(100*2, 1024, 4, 1, 0)\n",
    "        self.conv1 = UpConvBlock(1024, 512, 4, 2, 1, use_bn=True)\n",
    "        self.conv2 = UpConvBlock(512, 256, 4, 2, 1, use_bn=True)\n",
    "        self.conv3 = UpConvBlock(256, 128, 4, 2, 1, use_bn=True)\n",
    "        self.conv4 = UpConvBlock(128, 1, 4, 2, 1, use_bn=False, is_last=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, self.embed(y)], dim=1)\n",
    "\n",
    "        # out = self.fc(x)\n",
    "        # out = out.view(-1, 1024, 4, 4)\n",
    "\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        out = self.first_conv(x)\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(10, 100)\n",
    "y = torch.arange(10)\n",
    "model = Generator()\n",
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DownConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_bn=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.InstanceNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "        self.act = nn.LeakyReLU(2e-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed = nn.Embedding(10, 1*64*64)\n",
    "        self.conv1 = DownConvBlock(1*2, 128, 4, 2, 1, use_bn=False)\n",
    "        self.conv2 = DownConvBlock(128, 256, 4, 2, 1, use_bn=True)\n",
    "        self.conv3 = DownConvBlock(256, 512, 4, 2, 1, use_bn=True)\n",
    "        self.conv4 = DownConvBlock(512, 1024, 4, 2, 1, use_bn=True)\n",
    "        self.last_conv = nn.Conv2d(1024, 1, 4, 2, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, self.embed(y).view(-1, 1, 64, 64)], dim=1)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "\n",
    "        out = self.last_conv(out)\n",
    "        out = out.view(-1, 1)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(10, 1, 64, 64)\n",
    "y = torch.arange(10)\n",
    "model = Discriminator()\n",
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if type(m) in {\n",
    "            nn.Linear,\n",
    "            nn.Conv2d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.BatchNorm2d,\n",
    "            nn.Embedding,\n",
    "        }:\n",
    "            nn.init.normal_(m.weight, mean=0, std=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = Discriminator().to(device)\n",
    "model_g = Generator().to(device)\n",
    "init_weights(model_d), init_weights(model_g)\n",
    "\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "optimizer_d = optim.Adam(model_d.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_g = optim.Adam(model_g.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "img_size = 64\n",
    "num_channels = 1\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(num_channels)], [0.5 for _ in range(num_channels)]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "train_data = datasets.MNIST(root='mnist/train', train=True, transform=transform, download=False)\n",
    "val_data = datasets.MNIST(root='mnist/val', train=False, transform=transform, download=False)\n",
    "\n",
    "noise_dim = 100\n",
    "num_classes = 10\n",
    "fixed_noise = torch.randn((num_classes, noise_dim)).to(device)\n",
    "fixed_label = torch.arange(num_classes).to(device)\n",
    "\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer_d,\n",
    "            optimizer_g,\n",
    "            model_d,\n",
    "            model_g,\n",
    "            penalty_coeff,\n",
    "            device = device\n",
    "    ):\n",
    "        self.optimizer_d = optimizer_d\n",
    "        self.optimizer_g = optimizer_g\n",
    "        self.model_d = model_d\n",
    "        self.model_g = model_g\n",
    "        self.penalty_coeff = penalty_coeff\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def calc_grad_penalty(self, real, fake, label):\n",
    "        batch_size, num_channels, height, width = real.shape\n",
    "        epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, num_channels, height, width).to(self.device)\n",
    "\n",
    "        joint_distribution = epsilon*real + (1-epsilon)*fake\n",
    "        critic_term = self.model_d(joint_distribution, label)\n",
    "\n",
    "        gradient = torch.autograd.grad(\n",
    "            outputs = critic_term,\n",
    "            inputs = joint_distribution,\n",
    "            grad_outputs = torch.ones_like(critic_term),\n",
    "            retain_graph = True,\n",
    "            create_graph = True,\n",
    "        )[0].view(batch_size, -1)\n",
    "\n",
    "        l2_norm = torch.norm(gradient, p=2, dim=1)\n",
    "        grad_penalty = torch.mean((l2_norm - 1)**2)        \n",
    "\n",
    "        return grad_penalty\n",
    "\n",
    "\n",
    "    def calc_disc_loss(self, real, label, noise, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            real_arg = self.model_d(real, label)\n",
    "\n",
    "            fake = self.model_g(noise, label)\n",
    "            fake_arg = self.model_d(fake, label)\n",
    "\n",
    "            grad_penalty = self.calc_grad_penalty(real, fake, label)\n",
    "\n",
    "            loss_d = (torch.mean(fake_arg) - torch.mean(real_arg)) + self.penalty_coeff*grad_penalty\n",
    "\n",
    "        return loss_d\n",
    "\n",
    "\n",
    "    def calc_gen_loss(self, noise, label, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            fake = self.model_g(noise, label)\n",
    "            fake_arg = self.model_d(fake, label)\n",
    "            loss_g = - torch.mean(fake_arg)\n",
    "\n",
    "        return loss_g\n",
    "\n",
    "\n",
    "    def calc_metrics(self, metrics_dict, train_loader, val_loader):\n",
    "        self.model_d.eval(), self.model_g.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            final_str = ''\n",
    "            loaders_list = [('Train', train_loader), ('Val', val_loader)]\n",
    "            \n",
    "            if metrics_dict == None:\n",
    "                metrics_dict = {'Train': {'DiscLoss': [], 'GenLoss': []}, 'Val': {'DiscLoss': [], 'GenLoss': []}}\n",
    "\n",
    "            for name, loader in loaders_list:\n",
    "                len_data = 0\n",
    "                total_loss_d = 0\n",
    "                total_loss_g = 0\n",
    "\n",
    "                for real, label in loader:\n",
    "                    real, label = real.to(self.device), label.to(self.device)\n",
    "\n",
    "                    batch_size = real.shape[0]\n",
    "                    len_data += batch_size\n",
    "\n",
    "                    noise = torch.randn((batch_size, noise_dim)).to(self.device)\n",
    "\n",
    "                    loss_d = self.calc_disc_loss(real, label, noise, is_train=False)\n",
    "                    total_loss_d += loss_d\n",
    "\n",
    "                    loss_g = self.calc_gen_loss(noise, label, is_train=False)\n",
    "                    total_loss_g += loss_g\n",
    "                \n",
    "                disc_loss = total_loss_d/len_data\n",
    "                gen_loss = total_loss_g/len_data\n",
    "\n",
    "                final_str += ' -- {} Disc Loss: {:.5f} -- {} Gen Loss: {:.5f}'.format(name, disc_loss, name, gen_loss)\n",
    "                \n",
    "                metrics_dict[name]['DiscLoss'].append(disc_loss.item())\n",
    "                metrics_dict[name]['GenLoss'].append(gen_loss.item()) \n",
    "\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        return final_str, metrics_dict\n",
    "\n",
    "\n",
    "    def visualize_tensorboard(self, fixed_noise, fixed_label, epoch):\n",
    "        with torch.no_grad():\n",
    "            fake = self.model_g(fixed_noise, fixed_label)\n",
    "            fake_images = make_grid(fake, nrow=5, normalize=True)\n",
    "            writer_fake.add_image('Fake', fake_images, global_step=epoch)\n",
    "\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def fit(self, n_epochs, n_disc, train_loader, val_loader):\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        metrics_dict = None\n",
    "        steps = 1\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            for batch_idx, (real, label) in enumerate(train_loader):\n",
    "                real, label = real.to(self.device), label.to(self.device)\n",
    "\n",
    "                batch_size = real.shape[0]\n",
    "\n",
    "                for _ in range(n_disc):\n",
    "                    noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "                    loss_d = self.calc_disc_loss(real, label, noise, is_train=True)\n",
    "\n",
    "                    self.optimizer_d.zero_grad()\n",
    "                    loss_d.backward()\n",
    "                    self.optimizer_d.step()\n",
    "                \n",
    "                noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "                loss_g = self.calc_gen_loss(noise, label, is_train=True)\n",
    "\n",
    "                self.optimizer_g.zero_grad()\n",
    "                loss_g.backward()\n",
    "                self.optimizer_g.step()\n",
    "\n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f'Epoch: {epoch:2d}/{n_epochs} -- Batch: {batch_idx+1:3d}/{len(train_loader)}' + f' -- Train Disc Loss: {loss_d:.4f} -- Train Gen Loss: {loss_g:.4f}')\n",
    "                    self.visualize_tensorboard(fixed_noise, fixed_label, steps)\n",
    "                    steps += 1\n",
    "            \n",
    "            # if epoch == 1 or epoch%2 == 0:\n",
    "            #     final_str, metrics_dict = self.calc_metrics(metrics_dict, train_loader, val_loader)\n",
    "            #     print('Epoch: {:2d}'.format(epoch) + final_str)\n",
    "            \n",
    "        self.metrics_dict = metrics_dict\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/2 -- Batch:   1/469 -- Train Disc Loss: 550.3231 -- Train Gen Loss: 2.2336\n",
      "Epoch:  1/2 -- Batch:  21/469 -- Train Disc Loss: -49.2235 -- Train Gen Loss: 45.8081\n",
      "Epoch:  1/2 -- Batch:  41/469 -- Train Disc Loss: -100.4171 -- Train Gen Loss: 75.9870\n",
      "Epoch:  1/2 -- Batch:  61/469 -- Train Disc Loss: -104.1197 -- Train Gen Loss: 83.9225\n",
      "Epoch:  1/2 -- Batch:  81/469 -- Train Disc Loss: -102.6078 -- Train Gen Loss: 87.2601\n",
      "Epoch:  1/2 -- Batch: 101/469 -- Train Disc Loss: -98.0380 -- Train Gen Loss: 89.7384\n",
      "Epoch:  1/2 -- Batch: 121/469 -- Train Disc Loss: -92.2195 -- Train Gen Loss: 87.9884\n",
      "Epoch:  1/2 -- Batch: 141/469 -- Train Disc Loss: -88.5782 -- Train Gen Loss: 89.5317\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer_d, optimizer_g, model_d, model_g, penalty_coeff=10, device=device)\n",
    "trainer.fit(2, 2, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
