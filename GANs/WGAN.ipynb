{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, is_last=False, use_bn=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity() \n",
    "        self.act = nn.Tanh() if is_last else nn.ReLU()       \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed = nn.Embedding(10, 100)\n",
    "        self.fc = nn.Linear(100*2, 1024*4*4)\n",
    "        self.conv1 = UpConvBlock(1024, 512, 4, 2, 1, use_bn=True)\n",
    "        self.conv2 = UpConvBlock(512, 256, 4, 2, 1, use_bn=True)\n",
    "        self.conv3 = UpConvBlock(256, 128, 4, 2, 1, use_bn=True)\n",
    "        self.conv4 = UpConvBlock(128, 1, 4, 2, 1, use_bn=False, is_last=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, self.embed(y)], dim=1)\n",
    "\n",
    "        out = self.fc(x)\n",
    "        out = out.view(-1, 1024, 4, 4)\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(10, 100)\n",
    "y = torch.arange(10)\n",
    "model = Generator()\n",
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DownConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_bn=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "        self.act = nn.LeakyReLU(2e-1)\n",
    "        \n",
    "        nn.init.normal_(self.conv.weight, 0, 2e-2)\n",
    "        nn.init.normal_(self.bn.weight, 0, 2e-2) if use_bn else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed = nn.Embedding(10, 1*64*64)\n",
    "        self.conv1 = DownConvBlock(1*2, 128, 4, 2, 1, use_bn=False)\n",
    "        self.conv2 = DownConvBlock(128, 256, 4, 2, 1, use_bn=True)\n",
    "        self.conv3 = DownConvBlock(256, 512, 4, 2, 1, use_bn=True)\n",
    "        self.conv4 = DownConvBlock(512, 1024, 4, 2, 1, use_bn=True)\n",
    "        self.last_conv = nn.Conv2d(1024, 1, 4, 2, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, self.embed(y).view(-1, 1, 64, 64)], dim=1)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "\n",
    "        out = self.last_conv(out)\n",
    "        out = out.view(-1, 1)\n",
    "        out = F.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(10, 1, 64, 64)\n",
    "y = torch.arange(10)\n",
    "model = Discriminator()\n",
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if type(m) in {\n",
    "            nn.Linear,\n",
    "            nn.Conv2d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.BatchNorm2d,\n",
    "            nn.Embedding,\n",
    "        }:\n",
    "            nn.init.normal_(m.weight, mean=0, std=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = Discriminator().to(device)\n",
    "model_g = Generator().to(device)\n",
    "init_weights(model_d), init_weights(model_g)\n",
    "\n",
    "lr = 5e-4   # not choosing 5e-5 as it is too low\n",
    "optimizer_d = optim.RMSprop(model_d.parameters(), lr=lr)\n",
    "optimizer_g = optim.RMSprop(model_g.parameters(), lr=lr)\n",
    "\n",
    "img_size = 64\n",
    "num_channels = 1\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(num_channels)], [0.5 for _ in range(num_channels)]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "noise_dim = 100\n",
    "num_classes = 10\n",
    "\n",
    "train_data = datasets.MNIST(root='mnist/train', train=True, transform=transform, download=False)\n",
    "val_data = datasets.MNIST(root='mnist/val', train=False, transform=transform, download=False)\n",
    "\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')\n",
    "\n",
    "fixed_noise = torch.randn((num_classes, noise_dim)).to(device)\n",
    "fixed_label = torch.arange(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer_d,\n",
    "            optimizer_g,\n",
    "            model_d,\n",
    "            model_g,\n",
    "            device=device\n",
    "    ):\n",
    "        self.optimizer_d = optimizer_d\n",
    "        self.optimizer_g = optimizer_g\n",
    "        self.model_d = model_d\n",
    "        self.model_g = model_g\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def calc_disc_loss(self, real, label, noise, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            real_arg = self.model_d(real, label)\n",
    "\n",
    "            fake = self.model_g(noise, label)\n",
    "            fake_arg = self.model_d(fake, label)\n",
    "\n",
    "            loss_d = - (torch.mean(real_arg) - torch.mean(fake_arg))\n",
    "\n",
    "        return loss_d\n",
    "\n",
    "\n",
    "    def calc_gen_loss(self, noise, label, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            fake = self.model_g(noise, label)\n",
    "            fake_arg = self.model_d(fake, label)\n",
    "            loss_g = - torch.mean(fake_arg)\n",
    "\n",
    "        return loss_g\n",
    "\n",
    "\n",
    "    def calc_metrics(self, metrics_dict, train_loader, val_loader):\n",
    "        self.model_d.eval(), self.model_g.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            final_str = ''\n",
    "            loaders_list = [('Train', train_loader), ('Val', val_loader)]\n",
    "            \n",
    "            if metrics_dict == None:\n",
    "                metrics_dict = {'Train': {'DiscLoss': [], 'GenLoss': []}, 'Val': {'DiscLoss': [], 'GenLoss': []}}\n",
    "\n",
    "            for name, loader in loaders_list:\n",
    "                len_data = 0\n",
    "                total_loss_d = 0\n",
    "                total_loss_g = 0\n",
    "\n",
    "                for real, label in loader:\n",
    "                    real, label = real.to(self.device), label.to(self.device)\n",
    "\n",
    "                    batch_size = real.shape[0]\n",
    "                    len_data += batch_size\n",
    "\n",
    "                    noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "\n",
    "                    loss_d = self.calc_disc_loss(real, label, noise, is_train=False)\n",
    "                    total_loss_d += loss_d\n",
    "\n",
    "                    loss_g = self.calc_gen_loss(noise, label, is_train=False)\n",
    "                    total_loss_g += loss_g\n",
    "                \n",
    "                disc_loss = total_loss_d/len_data\n",
    "                gen_loss = total_loss_g/len_data\n",
    "\n",
    "                final_str += ' -- {} Disc Loss: {:.5f} -- {} Gen Loss: {:.5f}'.format(name, disc_loss, name, gen_loss)\n",
    "                \n",
    "                metrics_dict[name]['DiscLoss'].append(disc_loss.item())\n",
    "                metrics_dict[name]['GenLoss'].append(gen_loss.item()) \n",
    "\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        return final_str, metrics_dict\n",
    "\n",
    "\n",
    "    def visualize_tensorboard(self, fixed_noise, fixed_label, epoch):\n",
    "        with torch.no_grad():\n",
    "            fake = self.model_g(fixed_noise, fixed_label)\n",
    "            fake_images = make_grid(fake, nrow=5, normalize=True)\n",
    "            writer_fake.add_image('Fake', fake_images, global_step=epoch)\n",
    "\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def fit(self, n_epochs, n_disc, clip, train_loader, val_loader):\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        metrics_dict = None\n",
    "        steps = 1\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            for batch_idx, (real, label) in enumerate(train_loader):\n",
    "                real, label = real.to(self.device), label.to(self.device)\n",
    "\n",
    "                batch_size = real.shape[0]\n",
    "\n",
    "                for _ in range(n_disc):\n",
    "                    noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "                    loss_d = self.calc_disc_loss(real, label, noise, is_train=True)\n",
    "\n",
    "                    self.optimizer_d.zero_grad()\n",
    "                    loss_d.backward()\n",
    "                    self.optimizer_d.step()\n",
    "\n",
    "                    for w in model_d.parameters():\n",
    "                        torch.clamp_(w.data, -clip, +clip)\n",
    "                \n",
    "                noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "                loss_g = self.calc_gen_loss(noise, label, is_train=True)\n",
    "\n",
    "                self.optimizer_g.zero_grad()\n",
    "                loss_g.backward()\n",
    "                self.optimizer_g.step()\n",
    "\n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f'Epoch: {epoch:2d}/{n_epochs} -- Batch: {batch_idx+1:3d}/{len(train_loader)}' + f' -- Train Disc Loss: {loss_d:.4f} -- Train Gen Loss: {loss_g:.4f}')\n",
    "                    self.visualize_tensorboard(fixed_noise, fixed_label, steps)\n",
    "                    steps += 1\n",
    "            \n",
    "            if epoch == 1 or epoch%2 == 0:\n",
    "                final_str, metrics_dict = self.calc_metrics(metrics_dict, train_loader, val_loader)\n",
    "                print('Epoch: {:2d}'.format(epoch) + final_str)\n",
    "            \n",
    "        self.metrics_dict = metrics_dict\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/1 -- Batch:   1/938 -- Train Disc Loss: -0.0110 -- Train Gen Loss: -0.4979\n",
      "Epoch:  1/1 -- Batch:  21/938 -- Train Disc Loss: -0.0173 -- Train Gen Loss: -0.6584\n",
      "Epoch:  1/1 -- Batch:  41/938 -- Train Disc Loss: -0.0478 -- Train Gen Loss: -0.4462\n",
      "Epoch:  1/1 -- Batch:  61/938 -- Train Disc Loss: -0.0464 -- Train Gen Loss: -0.3196\n",
      "Epoch:  1/1 -- Batch:  81/938 -- Train Disc Loss: -0.0311 -- Train Gen Loss: -0.5164\n",
      "Epoch:  1/1 -- Batch: 101/938 -- Train Disc Loss: -0.1315 -- Train Gen Loss: -0.7618\n",
      "Epoch:  1/1 -- Batch: 121/938 -- Train Disc Loss: -0.0713 -- Train Gen Loss: -0.2727\n",
      "Epoch:  1/1 -- Batch: 141/938 -- Train Disc Loss: -0.0817 -- Train Gen Loss: -0.7342\n",
      "Epoch:  1/1 -- Batch: 161/938 -- Train Disc Loss: -0.0413 -- Train Gen Loss: -0.4780\n",
      "Epoch:  1/1 -- Batch: 181/938 -- Train Disc Loss: -0.0885 -- Train Gen Loss: -0.2299\n",
      "Epoch:  1/1 -- Batch: 201/938 -- Train Disc Loss: -0.1431 -- Train Gen Loss: -0.6740\n",
      "Epoch:  1/1 -- Batch: 221/938 -- Train Disc Loss: -0.1481 -- Train Gen Loss: -0.2606\n",
      "Epoch:  1/1 -- Batch: 241/938 -- Train Disc Loss: -0.1735 -- Train Gen Loss: -0.7080\n",
      "Epoch:  1/1 -- Batch: 261/938 -- Train Disc Loss: -0.0214 -- Train Gen Loss: -0.5507\n",
      "Epoch:  1/1 -- Batch: 281/938 -- Train Disc Loss: -0.1228 -- Train Gen Loss: -0.4945\n",
      "Epoch:  1/1 -- Batch: 301/938 -- Train Disc Loss: -0.0646 -- Train Gen Loss: -0.3381\n",
      "Epoch:  1/1 -- Batch: 321/938 -- Train Disc Loss: -0.1654 -- Train Gen Loss: -0.5387\n",
      "Epoch:  1/1 -- Batch: 341/938 -- Train Disc Loss: -0.0853 -- Train Gen Loss: -0.3100\n",
      "Epoch:  1/1 -- Batch: 361/938 -- Train Disc Loss: -0.0601 -- Train Gen Loss: -0.4034\n",
      "Epoch:  1/1 -- Batch: 381/938 -- Train Disc Loss: -0.1195 -- Train Gen Loss: -0.2647\n",
      "Epoch:  1/1 -- Batch: 401/938 -- Train Disc Loss: -0.0905 -- Train Gen Loss: -0.5553\n",
      "Epoch:  1/1 -- Batch: 421/938 -- Train Disc Loss: -0.0755 -- Train Gen Loss: -0.5177\n",
      "Epoch:  1/1 -- Batch: 441/938 -- Train Disc Loss: -0.1133 -- Train Gen Loss: -0.3031\n",
      "Epoch:  1/1 -- Batch: 461/938 -- Train Disc Loss: -0.0871 -- Train Gen Loss: -0.2624\n",
      "Epoch:  1/1 -- Batch: 481/938 -- Train Disc Loss: -0.1331 -- Train Gen Loss: -0.7233\n",
      "Epoch:  1/1 -- Batch: 501/938 -- Train Disc Loss: -0.0115 -- Train Gen Loss: -0.6736\n",
      "Epoch:  1/1 -- Batch: 521/938 -- Train Disc Loss: -0.2148 -- Train Gen Loss: -0.4131\n",
      "Epoch:  1/1 -- Batch: 541/938 -- Train Disc Loss: -0.0672 -- Train Gen Loss: -0.3624\n",
      "Epoch:  1/1 -- Batch: 561/938 -- Train Disc Loss: -0.1828 -- Train Gen Loss: -0.2738\n",
      "Epoch:  1/1 -- Batch: 581/938 -- Train Disc Loss: -0.0355 -- Train Gen Loss: -0.5226\n",
      "Epoch:  1/1 -- Batch: 601/938 -- Train Disc Loss: -0.1115 -- Train Gen Loss: -0.2811\n",
      "Epoch:  1/1 -- Batch: 621/938 -- Train Disc Loss: -0.0432 -- Train Gen Loss: -0.4472\n",
      "Epoch:  1/1 -- Batch: 641/938 -- Train Disc Loss: -0.1331 -- Train Gen Loss: -0.2507\n",
      "Epoch:  1/1 -- Batch: 661/938 -- Train Disc Loss: -0.0874 -- Train Gen Loss: -0.2830\n",
      "Epoch:  1/1 -- Batch: 681/938 -- Train Disc Loss: -0.1412 -- Train Gen Loss: -0.3063\n",
      "Epoch:  1/1 -- Batch: 701/938 -- Train Disc Loss: -0.1962 -- Train Gen Loss: -0.6418\n",
      "Epoch:  1/1 -- Batch: 721/938 -- Train Disc Loss: -0.1921 -- Train Gen Loss: -0.2868\n",
      "Epoch:  1/1 -- Batch: 741/938 -- Train Disc Loss: -0.1645 -- Train Gen Loss: -0.5486\n",
      "Epoch:  1/1 -- Batch: 761/938 -- Train Disc Loss: -0.2132 -- Train Gen Loss: -0.4487\n",
      "Epoch:  1/1 -- Batch: 781/938 -- Train Disc Loss: -0.0677 -- Train Gen Loss: -0.3295\n",
      "Epoch:  1/1 -- Batch: 801/938 -- Train Disc Loss: -0.2019 -- Train Gen Loss: -0.6293\n",
      "Epoch:  1/1 -- Batch: 821/938 -- Train Disc Loss: -0.1568 -- Train Gen Loss: -0.6285\n",
      "Epoch:  1/1 -- Batch: 841/938 -- Train Disc Loss: -0.1112 -- Train Gen Loss: -0.6505\n",
      "Epoch:  1/1 -- Batch: 861/938 -- Train Disc Loss: -0.0518 -- Train Gen Loss: -0.3561\n",
      "Epoch:  1/1 -- Batch: 881/938 -- Train Disc Loss: -0.1152 -- Train Gen Loss: -0.5103\n",
      "Epoch:  1/1 -- Batch: 901/938 -- Train Disc Loss: -0.1363 -- Train Gen Loss: -0.2509\n",
      "Epoch:  1/1 -- Batch: 921/938 -- Train Disc Loss: -0.1498 -- Train Gen Loss: -0.5209\n",
      "Epoch:  1 -- Train Disc Loss: -0.00186 -- Train Gen Loss: -0.00467 -- Val Disc Loss: -0.00176 -- Val Gen Loss: -0.00471\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer_d, optimizer_g, model_d, model_g, device)\n",
    "trainer.fit(1, 1, 1e-2, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
