{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class Down_ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.down_conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False, padding_mode='reflect')  \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.down_conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out) \n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Up_ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.do = nn.Dropout2d(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.up_conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class UNet_Generator(nn.Module):\n",
    "    def __init__(self, input_channels=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.first_down = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1, bias=False, padding_mode='reflect'), \n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "        self.down2 = Down_ConvBlock(64, 128)\n",
    "        self.down3 = Down_ConvBlock(128, 256)\n",
    "        self.down4 = Down_ConvBlock(256, 512)\n",
    "        self.down5 = Down_ConvBlock(512, 512)\n",
    "        self.down6 = Down_ConvBlock(512, 512)\n",
    "        self.down7 = Down_ConvBlock(512, 512)\n",
    "\n",
    "        self.last_down = Down_ConvBlock(512, 512)\n",
    "\n",
    "        self.first_up = Up_ConvBlock(512, 512)\n",
    "\n",
    "        self.up2 = Up_ConvBlock(512*2, 512)\n",
    "        self.up3 = Up_ConvBlock(512*2, 512)\n",
    "        self.up4 = Up_ConvBlock(512*2, 512)\n",
    "        self.up5 = Up_ConvBlock(512*2, 256)\n",
    "        self.up6 = Up_ConvBlock(256*2, 128)\n",
    "        self.up7 = Up_ConvBlock(128*2, 64)\n",
    "\n",
    "        self.last_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64*2, input_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down1 = self.first_down(x)\n",
    "\n",
    "        down2 = self.down2(down1)\n",
    "        down3 = self.down3(down2)\n",
    "        down4 = self.down4(down3)\n",
    "        down5 = self.down5(down4)\n",
    "        down6 = self.down6(down5)\n",
    "        down7 = self.down7(down6)\n",
    "\n",
    "        bottleneck = self.last_down(down7)\n",
    "        \n",
    "        up1 = self.first_up(bottleneck)\n",
    "\n",
    "        up2 = self.up2(torch.cat([down7, up1], dim=1))\n",
    "        up3 = self.up3(torch.cat([down6, up2], dim=1))\n",
    "        up4 = self.up4(torch.cat([down5, up3], dim=1))\n",
    "        up5 = self.up5(torch.cat([down4, up4], dim=1))\n",
    "        up6 = self.up6(torch.cat([down3, up5], dim=1))\n",
    "        up7 = self.up7(torch.cat([down2, up6], dim=1))\n",
    "\n",
    "        out = self.last_up(torch.cat([down1, up7], dim=1))\n",
    "        \n",
    "        return out\n",
    "\n",
    "x = torch.zeros(8, 3, 256, 256).to(device)\n",
    "model = UNet_Generator().to(device)\n",
    "output = model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 30, 30])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1, bias=False, padding_mode='reflect') \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Patch_Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.first_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels*2, 64, kernel_size=4, stride=2, padding=1, bias=False, padding_mode='reflect'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = ConvBlock(64, 128, 2)\n",
    "        self.conv3 = ConvBlock(128, 256, 2)\n",
    "        self.conv4 = ConvBlock(256, 512, 1)\n",
    "        self.last_conv = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1, bias=False, padding_mode='reflect')\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.first_conv(torch.cat([x, y], dim=1))\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.last_conv(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(8, 3, 256, 256).to(device)\n",
    "y = torch.zeros(8, 3, 256, 256).to(device)\n",
    "model = Patch_Discriminator().to(device)\n",
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    std = 2e-2\n",
    "    for m in model.modules():\n",
    "        if type(m) in {\n",
    "            nn.Conv2d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.Linear,\n",
    "        }:\n",
    "            nn.init.normal_(m.weight.data, mean=0.0, std=std)\n",
    "        if type(m) in {\n",
    "            nn.BatchNorm2d,\n",
    "        }:\n",
    "            nn.init.normal_(m.weight.data, mean=1.0, std=std)\n",
    "            nn.init.constant_(m.bias.data, val=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = Patch_Discriminator().to(device)\n",
    "model_g = UNet_Generator().to(device)\n",
    "init_weights(model_d), init_weights(model_g)\n",
    "\n",
    "lr = 2e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "optimizer_d = optim.Adam(model_d.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_g = optim.Adam(model_g.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "scaler_d = torch.cuda.amp.GradScaler()\n",
    "scaler_g = torch.cuda.amp.GradScaler()\n",
    "\n",
    "loss_fns = [nn.BCEWithLogitsLoss(), nn.L1Loss()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "num_channels = 3\n",
    "root_path = '/mnt/c/Users/121js/OneDrive/Desktop/TorchImages/maps/'\n",
    "\n",
    "common_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ],\n",
    "    additional_targets={'real_map': 'image'}\n",
    ")\n",
    "\n",
    "aerial_transform = A.Compose(\n",
    "    [\n",
    "        A.ColorJitter(p=0.5),\n",
    "        A.Normalize(\n",
    "            mean=[0.5 for _ in range(num_channels)],\n",
    "            std=[0.5 for _ in range(num_channels)],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(\n",
    "            mean=[0.5 for _ in range(num_channels)],\n",
    "            std=[0.5 for _ in range(num_channels)],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Aerial2Map(Dataset):\n",
    "    def __init__(self, data_path, common_transform, aerial_transform, map_transform):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.list_filenames = os.listdir(data_path)\n",
    "        self.common_transform = common_transform\n",
    "        self.aerial_transform = aerial_transform\n",
    "        self.map_transform = map_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_filename = self.list_filenames[index]\n",
    "        image_path = os.path.join(self.data_path, image_filename)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image_arr = np.array(image)\n",
    "\n",
    "        width = image_arr.shape[1]\n",
    "        split_at = width // 2\n",
    "        aerial, real_map = image_arr[:, :split_at, :], image_arr[:, split_at:, :]\n",
    "\n",
    "        common = self.common_transform(image=aerial, real_map=real_map)\n",
    "        aerial, real_map = common['image'], common['real_map']\n",
    "        aerial = self.aerial_transform(image=aerial)['image']\n",
    "        real_map = self.map_transform(image=real_map)['image']\n",
    "\n",
    "        return aerial, real_map\n",
    "    \n",
    "train_data = Aerial2Map(root_path+'train', common_transform, aerial_transform, map_transform)\n",
    "val_data = Aerial2Map(root_path+'val', common_transform, aerial_transform, map_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer_d,\n",
    "            optimizer_g,\n",
    "            scaler_d,\n",
    "            scaler_g,\n",
    "            model_d,\n",
    "            model_g,\n",
    "            loss_fns,\n",
    "            lambda_coeff,\n",
    "            device = device\n",
    "    ):\n",
    "        self.optimizer_d = optimizer_d\n",
    "        self.optimizer_g = optimizer_g\n",
    "        self.scaler_d = scaler_d\n",
    "        self.scaler_g = scaler_g\n",
    "        self.model_d = model_d\n",
    "        self.model_g = model_g\n",
    "        self.bce_loss_fn, self.l1_loss_fn = loss_fns\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def calc_disc_loss(self, aerial, real_map, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            real_args = self.model_d(aerial, real_map)\n",
    "            loss_real = self.bce_loss_fn(real_args, torch.ones_like(real_args))\n",
    "\n",
    "            fake_map = self.model_g(aerial)\n",
    "            fake_args = self.model_d(aerial, fake_map)\n",
    "            loss_fake = self.bce_loss_fn(fake_args, torch.zeros_like(fake_args))\n",
    "\n",
    "            loss_d = (loss_real + loss_fake) / 2\n",
    "\n",
    "        return loss_d\n",
    "    \n",
    "\n",
    "    def calc_gen_loss(self, aerial, real_map, is_train):\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            fake_map = self.model_g(aerial)\n",
    "            fake_args = self.model_d(aerial, fake_map)\n",
    "            loss_fake = self.bce_loss_fn(fake_args, torch.ones_like(fake_args))\n",
    "\n",
    "            loss_l1 = self.lambda_coeff * self.l1_loss_fn(fake_map, real_map)\n",
    "\n",
    "            loss_g = loss_fake + loss_l1\n",
    "        \n",
    "        return loss_g\n",
    "    \n",
    "    \n",
    "    def calc_metrics(self, metrics_dict, train_loader, val_loader):\n",
    "        self.model_d.eval(), self.model_g.eval()\n",
    "\n",
    "        final_str = ''\n",
    "        loaders_list = [('Train', train_loader), ('Val', val_loader)]\n",
    "            \n",
    "        if metrics_dict == None:\n",
    "            metrics_dict = {'Train': {'DiscLoss': [], 'GenLoss': []}, 'Val': {'DiscLoss': [], 'GenLoss': []}}\n",
    "\n",
    "        for name, loader in loaders_list:\n",
    "            len_data = 0\n",
    "            total_loss_d, total_loss_g = 0, 0\n",
    "\n",
    "            for aerial, real_map in loader:\n",
    "                aerial, real_map = aerial.to(device), real_map.to(device)\n",
    "                batch_size = aerial.shape[0]\n",
    "                len_data += batch_size\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_d = self.calc_disc_loss(aerial, real_map, is_train=False)\n",
    "                total_loss_d += loss_d\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_g = self.calc_gen_loss(aerial, real_map, is_train=False)\n",
    "                total_loss_g += loss_g\n",
    "                \n",
    "            disc_loss = total_loss_d/len_data\n",
    "            gen_loss = total_loss_g/len_data\n",
    "\n",
    "            final_str += ' -- {} Disc Loss: {:.5f} -- {} Gen Loss: {:.5f}'.format(name, disc_loss, name, gen_loss)\n",
    "                \n",
    "            metrics_dict[name]['DiscLoss'].append(disc_loss.item())\n",
    "            metrics_dict[name]['GenLoss'].append(gen_loss.item()) \n",
    "\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        return final_str, metrics_dict\n",
    "    \n",
    "\n",
    "    def visualize_tensorboard(self, aerial, real_map, writer_pix2pix, steps):\n",
    "        self.model_g.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_map = self.model_g(aerial)\n",
    "            combined_grid = torch.cat([\n",
    "                aerial[:4]*0.5 + 0.5,\n",
    "                real_map[:4]*0.5 + 0.5,\n",
    "                fake_map[:4]*0.5 + 0.5\n",
    "            ], dim=0\n",
    "            )\n",
    "            image_grid = make_grid(combined_grid, nrow=4, normalize=False)\n",
    "            writer_pix2pix.add_image('Fake', image_grid, global_step=steps)\n",
    "\n",
    "        self.model_g.train()\n",
    "\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def fit(self, n_epochs, train_loader, val_loader, writer_pix2pix):\n",
    "        self.model_d.train(), self.model_g.train()\n",
    "\n",
    "        metrics_dict = None\n",
    "        steps = 1\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            for batch_idx, (aerial, real_map) in enumerate(train_loader):\n",
    "                aerial, real_map = aerial.to(device), real_map.to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_d = self.calc_disc_loss(aerial, real_map, is_train=True)\n",
    "                    \n",
    "                self.optimizer_d.zero_grad()\n",
    "                self.scaler_d.scale(loss_d).backward()\n",
    "                self.scaler_d.step(self.optimizer_d)\n",
    "                self.scaler_d.update()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_g = self.calc_gen_loss(aerial, real_map, is_train=True)\n",
    "\n",
    "                self.optimizer_g.zero_grad()\n",
    "                self.scaler_g.scale(loss_g).backward()\n",
    "                self.scaler_g.step(self.optimizer_g)\n",
    "                self.scaler_g.update()\n",
    "\n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f'Epoch: {epoch:2d}/{n_epochs} -- Batch: {batch_idx+1:3d}/{len(train_loader)}' + f' -- Train Disc Loss: {loss_d:.4f} -- Train Gen Loss: {loss_g:.4f}')\n",
    "                    self.visualize_tensorboard(aerial, real_map, writer_pix2pix, steps)\n",
    "                    steps += 1\n",
    "            \n",
    "            final_str, metrics_dict = self.calc_metrics(metrics_dict, train_loader, val_loader)\n",
    "            print('Epoch: {:2d}'.format(epoch) + final_str)\n",
    "            \n",
    "        self.metrics_dict = metrics_dict\n",
    "    \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/50 -- Batch:   1/18 -- Train Disc Loss: 0.8361 -- Train Gen Loss: 80.7823\n",
      "Epoch:  1/50 -- Batch:   6/18 -- Train Disc Loss: 0.7163 -- Train Gen Loss: 80.8421\n",
      "Epoch:  1/50 -- Batch:  11/18 -- Train Disc Loss: 0.6510 -- Train Gen Loss: 70.3852\n",
      "Epoch:  1/50 -- Batch:  16/18 -- Train Disc Loss: 0.5390 -- Train Gen Loss: 59.2353\n"
     ]
    }
   ],
   "source": [
    "lambda_coeff = 100\n",
    "trainer = Trainer(optimizer_d, optimizer_g, scaler_d, scaler_g, model_d, model_g, loss_fns, lambda_coeff, device)\n",
    "\n",
    "n_epochs = 50\n",
    "writer_pix2pix = SummaryWriter('logs/pix2pix')\n",
    "trainer.fit(n_epochs, train_loader, val_loader, writer_pix2pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
