{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, hidden_dim, img_size, num_channels, num_classes=10, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed = nn.Embedding(num_classes, noise_dim)\n",
    "        self.fc1 = nn.Linear(noise_dim*2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, img_size*img_size*num_channels)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        leak = 1e-2\n",
    "\n",
    "        out = torch.cat([x, self.embed(y)], dim=1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = F.leaky_relu(out, leak)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = F.tanh(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(10, 100).to(device)\n",
    "y = torch.arange(10).to(device)\n",
    "model = Generator(100, 256, 28, 1).to(device)\n",
    "output = model(x, y)\n",
    "output.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size, num_channels, hidden_dim, noise_dim, num_classes=10, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size*num_channels)\n",
    "        self.fc1 = nn.Linear(img_size*img_size*num_channels*2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, noise_dim)\n",
    "        self.fc3 = nn.Linear(noise_dim, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        leak = 1e-2\n",
    "        \n",
    "        out = torch.cat([x, self.embed(y)], dim=1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = F.leaky_relu(out, leak)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = F.leaky_relu(out, leak)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "x = torch.zeros(10, 28*28*1).to(device)\n",
    "y = torch.arange(10).to(device)\n",
    "model = Discriminator(28, 1, 256, 100).to(device)\n",
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "num_channels = 1\n",
    "noise_dim = 100\n",
    "hidden_dim = 256\n",
    "num_classes = 10\n",
    "model_d = Discriminator(img_size, num_channels, hidden_dim, noise_dim, num_classes).to(device)\n",
    "model_g = Generator(noise_dim, hidden_dim, img_size, num_channels, num_classes).to(device)\n",
    "\n",
    "lr = 3e-4\n",
    "momentum = 0.9\n",
    "optimizer_d = optim.SGD(model_d.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer_g = optim.SGD(model_g.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(num_channels)], [0.5 for _ in range(num_channels)]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.MNIST(root='mnist/train', train=True, transform=transform, download=False)\n",
    "val_data = datasets.MNIST(root='mnist/val', train=False, transform=transform, download=False)\n",
    "\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')\n",
    "\n",
    "fixed_noise = torch.randn((num_classes, 100)).to(device)\n",
    "fixed_label = torch.arange(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_disc_loss(model_d, model_g, real, label, noise, loss_fn, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        real_arg = model_d(real, label)\n",
    "        loss_real = loss_fn(real_arg, torch.ones_like(real_arg))\n",
    "        \n",
    "        fake = model_g(noise, label)\n",
    "        fake_arg = model_d(fake, label)\n",
    "        loss_fake = loss_fn(fake_arg, torch.zeros_like(fake_arg))\n",
    "\n",
    "        loss_d = loss_fake + loss_real\n",
    "\n",
    "    return loss_d\n",
    "\n",
    "def calc_gen_loss(model_d, model_g, noise, label, loss_fn, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        fake = model_g(noise, label)\n",
    "        fake_arg = model_d(fake, label)\n",
    "        loss_g = loss_fn(fake_arg, torch.ones_like(fake_arg))\n",
    "\n",
    "    return loss_g\n",
    "\n",
    "def calc_metrics(model_d, model_g, loss_fn, train_loader, val_loader, metrics_dict, device=device):\n",
    "    with torch.no_grad():\n",
    "        final_str = ''\n",
    "        loaders_list = [('Train', train_loader), ('Val', val_loader)]\n",
    "        \n",
    "        if metrics_dict == None:\n",
    "            metrics_dict = {'Train': {'DiscLoss': [], 'GenLoss': []}, 'Val': {'DiscLoss': [], 'GenLoss': []}}\n",
    "\n",
    "        for name, loader in loaders_list:\n",
    "            len_data = 0\n",
    "            total_loss_d = 0\n",
    "            total_loss_g = 0\n",
    "\n",
    "            for real, label in loader:\n",
    "                real, label = real.to(device), label.to(device)\n",
    "\n",
    "                batch_size = real.shape[0]\n",
    "                len_data += batch_size\n",
    "\n",
    "                real = real.view(batch_size, img_size*img_size*num_channels)\n",
    "                noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "\n",
    "                loss_d = calc_disc_loss(model_d, model_g, real, label, noise, loss_fn, is_train=False)\n",
    "                total_loss_d += loss_d\n",
    "\n",
    "                loss_g = calc_gen_loss(model_d, model_g, noise, label, loss_fn, is_train=False)\n",
    "                total_loss_g += loss_g\n",
    "            \n",
    "            disc_loss = total_loss_d/len_data\n",
    "            gen_loss = total_loss_g/len_data\n",
    "\n",
    "            final_str += ' -- {} Disc Loss: {:.5f} -- {} Gen Loss: {:.5f}'.format(name, disc_loss, name, gen_loss)\n",
    "            \n",
    "            metrics_dict[name]['DiscLoss'].append(disc_loss.item())\n",
    "            metrics_dict[name]['GenLoss'].append(gen_loss.item()) \n",
    "    \n",
    "    return final_str, metrics_dict\n",
    "\n",
    "def visualize_tensorboard(model_g, fixed_noise, fixed_label, epoch):    \n",
    "    with torch.no_grad():\n",
    "        fake = model_g(fixed_noise, fixed_label).view(-1, num_channels, img_size, img_size)\n",
    "        fake_images = make_grid(fake, nrow=5, normalize=True)\n",
    "        writer_fake.add_image('Fake', fake_images, global_step=epoch)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "def training_loop(n_epochs, disc_iter, optimizer_d, optimizer_g, model_d, model_g, train_loader, val_loader, device=device):\n",
    "    metrics_dict = None\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        for real, label in train_loader:\n",
    "            real, label = real.to(device), label.to(device)\n",
    "            \n",
    "            batch_size = real.shape[0]\n",
    "            real = real.view(batch_size, img_size*img_size*num_channels)\n",
    "\n",
    "            for _ in range(disc_iter):\n",
    "                noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "                loss_d = calc_disc_loss(model_d, model_g, real, label, noise, loss_fn, is_train=True)\n",
    "\n",
    "                optimizer_d.zero_grad()\n",
    "                loss_d.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "            noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "            loss_g = calc_gen_loss(model_d, model_g, noise, label, loss_fn, is_train=True)\n",
    "\n",
    "            optimizer_g.zero_grad()\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "        if epoch == 1 or epoch%2 == 0:\n",
    "            final_str, metrics_dict = calc_metrics(model_d, model_g, loss_fn, train_loader, val_loader, metrics_dict, device)\n",
    "            print('Epoch: {:3d}'.format(epoch) + final_str)\n",
    "            \n",
    "        visualize_tensorboard(model_g, fixed_noise, fixed_label, epoch)\n",
    "\n",
    "    return model_d, model_g, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 -- Train Disc Loss: 0.00037 -- Train Gen Loss: 0.00368 -- Val Disc Loss: 0.00038 -- Val Gen Loss: 0.00375\n",
      "Epoch:   2 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00694 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00706\n",
      "Epoch:   4 -- Train Disc Loss: 0.00002 -- Train Gen Loss: 0.00934 -- Val Disc Loss: 0.00002 -- Val Gen Loss: 0.00949\n",
      "Epoch:   6 -- Train Disc Loss: 0.00001 -- Train Gen Loss: 0.01015 -- Val Disc Loss: 0.00001 -- Val Gen Loss: 0.01032\n",
      "Epoch:   8 -- Train Disc Loss: 0.00002 -- Train Gen Loss: 0.00980 -- Val Disc Loss: 0.00002 -- Val Gen Loss: 0.00996\n",
      "Epoch:  10 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00793 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00806\n",
      "Epoch:  12 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00742 -- Val Disc Loss: 0.00008 -- Val Gen Loss: 0.00754\n",
      "Epoch:  14 -- Train Disc Loss: 0.00016 -- Train Gen Loss: 0.00638 -- Val Disc Loss: 0.00016 -- Val Gen Loss: 0.00648\n",
      "Epoch:  16 -- Train Disc Loss: 0.00089 -- Train Gen Loss: 0.00335 -- Val Disc Loss: 0.00090 -- Val Gen Loss: 0.00341\n",
      "Epoch:  18 -- Train Disc Loss: 0.00082 -- Train Gen Loss: 0.00396 -- Val Disc Loss: 0.00083 -- Val Gen Loss: 0.00404\n",
      "Epoch:  20 -- Train Disc Loss: 0.00015 -- Train Gen Loss: 0.00733 -- Val Disc Loss: 0.00015 -- Val Gen Loss: 0.00745\n",
      "Epoch:  22 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00809 -- Val Disc Loss: 0.00008 -- Val Gen Loss: 0.00823\n",
      "Epoch:  24 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00820 -- Val Disc Loss: 0.00008 -- Val Gen Loss: 0.00834\n",
      "Epoch:  26 -- Train Disc Loss: 0.00002 -- Train Gen Loss: 0.01111 -- Val Disc Loss: 0.00002 -- Val Gen Loss: 0.01130\n",
      "Epoch:  28 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00785 -- Val Disc Loss: 0.00008 -- Val Gen Loss: 0.00799\n",
      "Epoch:  30 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00880 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00895\n",
      "Epoch:  32 -- Train Disc Loss: 0.00009 -- Train Gen Loss: 0.00748 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.00760\n",
      "Epoch:  34 -- Train Disc Loss: 0.00014 -- Train Gen Loss: 0.00706 -- Val Disc Loss: 0.00014 -- Val Gen Loss: 0.00718\n",
      "Epoch:  36 -- Train Disc Loss: 0.00015 -- Train Gen Loss: 0.00726 -- Val Disc Loss: 0.00015 -- Val Gen Loss: 0.00738\n",
      "Epoch:  38 -- Train Disc Loss: 0.00010 -- Train Gen Loss: 0.00758 -- Val Disc Loss: 0.00010 -- Val Gen Loss: 0.00771\n",
      "Epoch:  40 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00883 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00898\n",
      "Epoch:  42 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00908 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00923\n",
      "Epoch:  44 -- Train Disc Loss: 0.00003 -- Train Gen Loss: 0.00966 -- Val Disc Loss: 0.00003 -- Val Gen Loss: 0.00982\n",
      "Epoch:  46 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00845 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00859\n",
      "Epoch:  48 -- Train Disc Loss: 0.00009 -- Train Gen Loss: 0.00843 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.00857\n",
      "Epoch:  50 -- Train Disc Loss: 0.00012 -- Train Gen Loss: 0.00702 -- Val Disc Loss: 0.00012 -- Val Gen Loss: 0.00714\n",
      "Epoch:  52 -- Train Disc Loss: 0.00012 -- Train Gen Loss: 0.00707 -- Val Disc Loss: 0.00011 -- Val Gen Loss: 0.00719\n",
      "Epoch:  54 -- Train Disc Loss: 0.00020 -- Train Gen Loss: 0.00628 -- Val Disc Loss: 0.00020 -- Val Gen Loss: 0.00639\n",
      "Epoch:  56 -- Train Disc Loss: 0.00016 -- Train Gen Loss: 0.00694 -- Val Disc Loss: 0.00016 -- Val Gen Loss: 0.00705\n",
      "Epoch:  58 -- Train Disc Loss: 0.00011 -- Train Gen Loss: 0.00683 -- Val Disc Loss: 0.00011 -- Val Gen Loss: 0.00694\n",
      "Epoch:  60 -- Train Disc Loss: 0.00011 -- Train Gen Loss: 0.00688 -- Val Disc Loss: 0.00011 -- Val Gen Loss: 0.00700\n",
      "Epoch:  62 -- Train Disc Loss: 0.00017 -- Train Gen Loss: 0.00719 -- Val Disc Loss: 0.00016 -- Val Gen Loss: 0.00732\n",
      "Epoch:  64 -- Train Disc Loss: 0.00016 -- Train Gen Loss: 0.00696 -- Val Disc Loss: 0.00016 -- Val Gen Loss: 0.00707\n",
      "Epoch:  66 -- Train Disc Loss: 0.00013 -- Train Gen Loss: 0.00781 -- Val Disc Loss: 0.00013 -- Val Gen Loss: 0.00793\n",
      "Epoch:  68 -- Train Disc Loss: 0.00009 -- Train Gen Loss: 0.00767 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.00779\n",
      "Epoch:  70 -- Train Disc Loss: 0.00014 -- Train Gen Loss: 0.00741 -- Val Disc Loss: 0.00013 -- Val Gen Loss: 0.00753\n",
      "Epoch:  72 -- Train Disc Loss: 0.00014 -- Train Gen Loss: 0.00700 -- Val Disc Loss: 0.00014 -- Val Gen Loss: 0.00712\n",
      "Epoch:  74 -- Train Disc Loss: 0.00012 -- Train Gen Loss: 0.00814 -- Val Disc Loss: 0.00011 -- Val Gen Loss: 0.00826\n",
      "Epoch:  76 -- Train Disc Loss: 0.00009 -- Train Gen Loss: 0.00742 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.00754\n",
      "Epoch:  78 -- Train Disc Loss: 0.00015 -- Train Gen Loss: 0.00713 -- Val Disc Loss: 0.00015 -- Val Gen Loss: 0.00727\n",
      "Epoch:  80 -- Train Disc Loss: 0.00009 -- Train Gen Loss: 0.00808 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.00822\n",
      "Epoch:  82 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00927 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00943\n",
      "Epoch:  84 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.00946 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.00962\n",
      "Epoch:  86 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00870 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00885\n",
      "Epoch:  88 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00812 -- Val Disc Loss: 0.00008 -- Val Gen Loss: 0.00826\n",
      "Epoch:  90 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00815 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00829\n",
      "Epoch:  92 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.00991 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.01006\n",
      "Epoch:  94 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00847 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00862\n",
      "Epoch:  96 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00849 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00863\n",
      "Epoch:  98 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00809 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00820\n",
      "Epoch: 100 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00828 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00842\n",
      "Epoch: 102 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00900 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00916\n",
      "Epoch: 104 -- Train Disc Loss: 0.00003 -- Train Gen Loss: 0.01039 -- Val Disc Loss: 0.00003 -- Val Gen Loss: 0.01057\n",
      "Epoch: 106 -- Train Disc Loss: 0.00003 -- Train Gen Loss: 0.00904 -- Val Disc Loss: 0.00003 -- Val Gen Loss: 0.00919\n",
      "Epoch: 108 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00932 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00947\n",
      "Epoch: 110 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00850 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00865\n",
      "Epoch: 112 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.00983 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.00999\n",
      "Epoch: 114 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00901 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00916\n",
      "Epoch: 116 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00864 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00878\n",
      "Epoch: 118 -- Train Disc Loss: 0.00002 -- Train Gen Loss: 0.01116 -- Val Disc Loss: 0.00002 -- Val Gen Loss: 0.01134\n",
      "Epoch: 120 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.00888 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.00902\n",
      "Epoch: 122 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.00960 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.00975\n",
      "Epoch: 124 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00996 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01014\n",
      "Epoch: 126 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00983 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01001\n",
      "Epoch: 128 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01045 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01062\n",
      "Epoch: 130 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00978 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00993\n",
      "Epoch: 132 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01039 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01057\n",
      "Epoch: 134 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.01083 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01100\n",
      "Epoch: 136 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00942 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.00955\n",
      "Epoch: 138 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01023 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01043\n",
      "Epoch: 140 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01178 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01196\n",
      "Epoch: 142 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00978 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00991\n",
      "Epoch: 144 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.01003 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.01019\n",
      "Epoch: 146 -- Train Disc Loss: 0.00003 -- Train Gen Loss: 0.01136 -- Val Disc Loss: 0.00003 -- Val Gen Loss: 0.01157\n",
      "Epoch: 148 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.01016 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.01035\n",
      "Epoch: 150 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00916 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00931\n",
      "Epoch: 152 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01046 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01063\n",
      "Epoch: 154 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.01023 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.01037\n",
      "Epoch: 156 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00980 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00995\n",
      "Epoch: 158 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00987 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01002\n",
      "Epoch: 160 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.01071 -- Val Disc Loss: 0.00004 -- Val Gen Loss: 0.01091\n",
      "Epoch: 162 -- Train Disc Loss: 0.00003 -- Train Gen Loss: 0.01123 -- Val Disc Loss: 0.00003 -- Val Gen Loss: 0.01142\n",
      "Epoch: 164 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.00986 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01006\n",
      "Epoch: 166 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.01015 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01033\n",
      "Epoch: 168 -- Train Disc Loss: 0.00004 -- Train Gen Loss: 0.01055 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01069\n",
      "Epoch: 170 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00866 -- Val Disc Loss: 0.00008 -- Val Gen Loss: 0.00879\n",
      "Epoch: 172 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.01191 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.01206\n",
      "Epoch: 174 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01004 -- Val Disc Loss: 0.00005 -- Val Gen Loss: 0.01022\n",
      "Epoch: 176 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.00978 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00994\n",
      "Epoch: 178 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.01124 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01138\n",
      "Epoch: 180 -- Train Disc Loss: 0.00008 -- Train Gen Loss: 0.00866 -- Val Disc Loss: 0.00009 -- Val Gen Loss: 0.00880\n",
      "Epoch: 182 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00914 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00929\n",
      "Epoch: 184 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00934 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00948\n",
      "Epoch: 186 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.01014 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01032\n",
      "Epoch: 188 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.00962 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.00981\n",
      "Epoch: 190 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.01043 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.01065\n",
      "Epoch: 192 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01003 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01025\n",
      "Epoch: 194 -- Train Disc Loss: 0.00005 -- Train Gen Loss: 0.01018 -- Val Disc Loss: 0.00006 -- Val Gen Loss: 0.01035\n",
      "Epoch: 196 -- Train Disc Loss: 0.00006 -- Train Gen Loss: 0.01125 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.01142\n",
      "Epoch: 198 -- Train Disc Loss: 0.00007 -- Train Gen Loss: 0.00976 -- Val Disc Loss: 0.00007 -- Val Gen Loss: 0.00991\n",
      "Epoch: 200 -- Train Disc Loss: 0.00009 -- Train Gen Loss: 0.01012 -- Val Disc Loss: 0.00010 -- Val Gen Loss: 0.01028\n"
     ]
    }
   ],
   "source": [
    "model_d, model_g, metrics_dict = training_loop(200, 1, optimizer_d, optimizer_g, model_d, model_g, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
